# Autonomous_Navigation

SLAM uses a complex array of computations,algorithms and sensory inputs to navigate in a previously unknown environment or revise a known map.SLAM is the mapping of an environment using the continual interplay between the mapping device, the robot, and the location it is in.The robot not only maps the area as it interacts with it, but it also determines its own position at the same time.The robot or unmanned vehicle is at the centre of the problem-solving process.The robot used must have superior odometry capabilities.Odometry is a measurement of a robot's ability to estimate its own position.Normally, the robot calculates this based on the position of its wheels. 

To begin with, consider odometry to be the base which helps the robot to navigate. A URDF file of the robot base is created considering the 3 wheels to be child links and base to be the parent link to simulate it in gazebo and RVIZ.The URDF generated along with kinematics is spawned to map a known environment. Once the map is generated it is saved and fed to the humanoid. The robot is equipped with a Rpi - LiDAR(A2 model) mounted just below the torso.RPLIDAR A2's core rotates clockwise to perform 360-degree omnidirectional laser range scanning for its surroundings and then build an outline map.RPLIDAR A2 reduces mechanical friction in operating by using a self-designed brushless motor, as opposed to the usual belt drive mode. As a result, the RPLIDAR A2 can run smoothly without any disturbance.RPLIDAR extends the range radius by 16 metres thanks to algorithm optimization, resulting in more data due to a larger environmental outline map.The RPLIDAR A2 uses the laser triangulation ranging method, and it measures distance data 8000 times per second with the high-speed RPVision range engine. It also has exceptional long-range capability. The model comes with a support of ROS wrapper called rplidar_ros package which has a set of predefined algorithms to launch and operate the LiDAR. Once the rplidar launch file is launched, the LiDAR core rotates 360 - degree scanning the environment and the humanoid starts travelling in the environment. The LiDAR provides laser data on the topic /scan which has a parameter RANGES. It is the list of 720 values representing depth measured at each degree shift.

In order to achieve obstacle avoidance, divide these values into 3 regions namely front, right and left. These regions are divided amongst the 180-degree region of the humanoid considering only the front half of its circumference. This narrows down the possibility of the robot to move in multiple directions by specifying a certain direction when an obstacle is found in a particular region amongst the 3 defined. With these constraints specified and allowing a particular threshold the robot starts navigating from its initial position performing obstacle avoidance by simultaneous localisation of its current pose in the map generated earlier considering it to be the local map and maps its surroundings which further generates the global map.The robot is employed with a wall follower algorithm which along with avoiding obstacles can reach the goal in the mapped environment.
